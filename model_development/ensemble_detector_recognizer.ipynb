{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7fe29c157df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Installing required dependencies \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from tqdm import tqdm\n",
    "plt.ion() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\n",
      "To: /root/work/test_cv_project/artifacts/face_detection_yunet_2023mar.onnx\n",
      "100%|██████████| 233k/233k [00:00<00:00, 2.21MB/s]\n"
     ]
    }
   ],
   "source": [
    "# pylint: disable=C0301\n",
    "url = \"https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\"\n",
    "file_name = \"face_detection_yunet_2023mar.onnx\"\n",
    "home = \"../artifacts\"\n",
    "output = os.path.join(home, file_name)\n",
    "\n",
    "if os.path.isfile(output) is False:\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "try:\n",
    "    face_detector = cv2.FaceDetectorYN_create(\n",
    "        output, \"\", (0, 0)\n",
    "    )\n",
    "except Exception as err:\n",
    "    raise ValueError(\n",
    "        \"Exception while calling opencv.FaceDetectorYN_create module.\"\n",
    "        + \"This is an optional dependency.\"\n",
    "        + \"You can install it as pip install opencv-contrib-python.\"\n",
    "    ) from err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_face(img):\n",
    "    score_threshold = float(os.environ.get(\"yunet_score_threshold\", \"0.9\"))\n",
    "    resp = []\n",
    "    faces = []\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    # resize image if it is too large (Yunet fails to detect faces on large input sometimes)\n",
    "    # I picked 640 as a threshold because it is the default value of max_size in Yunet.\n",
    "    resized = False\n",
    "    r = 1  # resize factor\n",
    "    if height > 640 or width > 640:\n",
    "        r = 640.0 / max(height, width)\n",
    "        img = cv2.resize(img, (int(width * r), int(height * r)))\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "        resized = True\n",
    "    face_detector.setInputSize((width, height))\n",
    "    face_detector.setScoreThreshold(score_threshold)\n",
    "    _, faces = face_detector.detect(img)\n",
    "    if faces is None:\n",
    "        print(\"amogus\")\n",
    "    for face in faces:\n",
    "        # pylint: disable=W0105\n",
    "        \"\"\"\n",
    "        The detection output faces is a two-dimension array of type CV_32F,\n",
    "        whose rows are the detected face instances, columns are the location\n",
    "        of a face and 5 facial landmarks.\n",
    "        The format of each row is as follows:\n",
    "        x1, y1, w, h, x_re, y_re, x_le, y_le, x_nt, y_nt,\n",
    "        x_rcm, y_rcm, x_lcm, y_lcm,\n",
    "        where x1, y1, w, h are the top-left coordinates, width and height of\n",
    "        the face bounding box,\n",
    "        {x, y}_{re, le, nt, rcm, lcm} stands for the coordinates of right eye,\n",
    "        left eye, nose tip, the right corner and left corner of the mouth respectively.\n",
    "        \"\"\"\n",
    "        (x, y, w, h, x_le, y_le, x_re, y_re) = list(map(int, face[:8]))\n",
    "\n",
    "        # YuNet returns negative coordinates if it thinks part of the detected face\n",
    "        # is outside the frame.\n",
    "        x = max(x, 0)\n",
    "        y = max(y, 0)\n",
    "        if resized:\n",
    "            x, y, w, h = int(x / r), int(y / r), int(w / r), int(h / r)\n",
    "            x_re, y_re, x_le, y_le = (\n",
    "                int(x_re / r),\n",
    "                int(y_re / r),\n",
    "                int(x_le / r),\n",
    "                int(y_le / r),\n",
    "            )\n",
    "        confidence = float(face[-1])\n",
    "\n",
    "        facial_area = dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            w=w,\n",
    "            h=h,\n",
    "            confidence=confidence,\n",
    "            left_eye=(x_re, y_re),\n",
    "            right_eye=(x_le, y_le),\n",
    "        )\n",
    "        resp.append(facial_area)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(img, face):\n",
    "    # Crop the face from the image\n",
    "    x, y, w, h = face[\"x\"], face[\"y\"], face[\"w\"], face[\"h\"], \n",
    "    face_img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Resize the cropped face to 256x256\n",
    "    face_img = cv2.resize(face_img, (256, 256))\n",
    "\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model_ft = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# print(model_ft)\n",
    "\n",
    "num_ftrs = model_ft._fc.in_features\n",
    "# model_ft._fc = nn.Linear(num_ftrs, 7)\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(1280,512)),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('dropout', nn.Dropout(0.4)),\n",
    "    ('fc2', nn.Linear(512,7))\n",
    "#     ('output', nn.Softmax(dim=1))\n",
    "]))\n",
    "model_ft._fc = fc\n",
    "model_ft.load_state_dict(torch.load(\"../artifacts/efficientnet_emotion.pt\"))\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "def predict_emotion(face_img):\n",
    "    # Preprocess the face image\n",
    "    face_img = transform(face_img).unsqueeze(0)\n",
    "\n",
    "    # Run the emotion prediction model\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ft(face_img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"2FE9.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': 226,\n",
       "  'y': 49,\n",
       "  'w': 118,\n",
       "  'h': 192,\n",
       "  'confidence': 0.9388536810874939,\n",
       "  'left_eye': (314, 121),\n",
       "  'right_eye': (258, 126)}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces = predict_face(img)\n",
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "croped_faces = [crop_face(img, face) for face in faces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fear']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = [predict_emotion(croped_face) for croped_face in croped_faces]\n",
    "emotion_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emotions_named = [emotion_names[emotion_num] for emotion_num in emotions]\n",
    "emotions_named"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
